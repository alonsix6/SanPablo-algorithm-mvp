name: UCSP Algorithm - Daily Incremental Update

# Actualización incremental diaria: trae últimos 7 días y mergea (~3 min)
# Corre martes-domingo. El lunes corre el full rebuild semanal.
on:
  # Ejecutar martes a domingo a las 1 PM UTC (8 AM Perú)
  # Días: 0=Dom, 2=Mar, 3=Mié, 4=Jue, 5=Vie, 6=Sáb (Lunes=1 es el full rebuild)
  schedule:
    - cron: '0 13 * * 0,2-6'

  # Permitir ejecución manual
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-commit:
    name: Incremental update - all sources (daily)
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      # 1. Checkout del código (incluyendo data existente para merge)
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      # 2. Setup Node.js 20
      - name: Setup Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'scrapers/package-lock.json'

      # 3. Instalar dependencias
      - name: Install dependencies
        working-directory: ./scrapers
        run: npm ci

      # 4. Crear directorios de datos
      - name: Create data directories
        run: |
          mkdir -p data/trends data/tiktok data/meta data/hubspot data/ga4 data/powerbi
          mkdir -p public/data/trends public/data/tiktok public/data/meta public/data/hubspot public/data/ga4 public/data/powerbi

      # 5. Ejecutar scraper de Google Trends (Apify)
      - name: Run Google Trends Scraper
        working-directory: ./scrapers
        run: |
          echo "Scraping Google Trends via Apify..."
          node google_trends_apify.js --client=ucsp
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        continue-on-error: true

      # 6. Ejecutar scraper de TikTok (Apify)
      - name: Run TikTok Trends Scraper
        working-directory: ./scrapers
        run: |
          echo "Scraping TikTok Trends via Apify..."
          node tiktok_apify.js --client=ucsp
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        continue-on-error: true

      # 7. Ejecutar scraper de Meta/Facebook (Apify)
      - name: Run Meta/Facebook Scraper
        working-directory: ./scrapers
        run: |
          echo "Scraping Facebook Pages via Apify..."
          node meta_apify.js --client=ucsp
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        continue-on-error: true

      # 8. Ejecutar HubSpot CRM Connector (INCREMENTAL - 7 días, merge)
      - name: Run HubSpot CRM Connector (incremental)
        working-directory: ./scrapers
        run: |
          echo "HubSpot CRM incremental update (7 dias)..."
          node hubspot_api.js --client=ucsp --mode=incremental
        env:
          HUBSPOT_ACCESS_TOKEN: ${{ secrets.HUBSPOT_ACCESS_TOKEN }}
        continue-on-error: true

      # 9. Ejecutar GA4 Data Connector (INCREMENTAL - 7 días, merge)
      - name: Run GA4 Data Connector (incremental)
        working-directory: ./scrapers
        run: |
          echo "GA4 Analytics incremental update (7 dias)..."
          node ga4_api.js --client=ucsp --mode=incremental
        env:
          GA4_PROPERTY_ID: ${{ secrets.GA4_PROPERTY_ID }}
          GA4_CREDENTIALS_JSON: ${{ secrets.GA4_CREDENTIALS_JSON }}
        continue-on-error: true

      # 10. Ejecutar Power BI Connector (INCREMENTAL)
      - name: Run Power BI Connector (incremental)
        working-directory: ./scrapers
        run: |
          echo "Power BI incremental update (7 dias)..."
          node powerbi_api.js --client=ucsp --mode=incremental
        env:
          POWERBI_CLIENT_ID: ${{ secrets.POWERBI_CLIENT_ID }}
          POWERBI_CLIENT_SECRET: ${{ secrets.POWERBI_CLIENT_SECRET }}
          POWERBI_TENANT_ID: ${{ secrets.POWERBI_TENANT_ID }}
        continue-on-error: true

      # 11. Ejecutar ML Pipeline
      - name: Run ML Pipeline
        run: |
          echo "Ejecutando ML Pipeline..."
          node ml/pipeline/weekly_pipeline.js
        continue-on-error: true

      # 12. Verificar archivos generados
      - name: Check generated files
        run: |
          echo "Verificando archivos generados..."
          echo "=== HubSpot ===" && ls -lah data/hubspot/ || echo "No data"
          echo "=== GA4 ===" && ls -lah data/ga4/ || echo "No data"
          echo "=== Power BI ===" && ls -lah data/powerbi/ || echo "No data"

      # 13. Commit y push de los datos
      - name: Commit and push data
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "UCSP Algorithm Bot"

          git add data/ public/data/

          if git diff --staged --quiet; then
            echo "No hay cambios en los datos"
          else
            TIMESTAMP=$(date +'%Y-%m-%d %H:%M UTC')
            git commit -m "Daily incremental update - $TIMESTAMP"
            git push
            echo "Datos UCSP actualizados (incremental)"
          fi

      # 14. Notificación
      - name: Status notification
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "UCSP Algorithm daily update completado"
            echo "Proxima ejecucion: Manana 8 AM (Peru)"
          else
            echo "Algunos scrapers fallaron - revisa los logs"
          fi
